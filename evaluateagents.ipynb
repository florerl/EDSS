{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from sb3_contrib import RecurrentPPO\n",
    "import edss\n",
    "\n",
    "class BayesEstimator():\n",
    "    def __init__(self, env: gym.Env, transform=None) -> None:\n",
    "        self.buildActions: list[edss.Action] = []\n",
    "        self.testActions: list[edss.Action] = []\n",
    "        for action in env.unwrapped.actions:\n",
    "            match action.type:\n",
    "                case \"build\":\n",
    "                    self.buildActions.append(action)\n",
    "                case \"test\":\n",
    "                    self.testActions.append(action)\n",
    "                case _:\n",
    "                    pass\n",
    "        self.transform = transform\n",
    "        self.features = np.zeros_like(env.unwrapped.features, dtype=np.single)\n",
    "\n",
    "    def estimate(self, observation):\n",
    "        self.features = np.zeros_like(self.features)\n",
    "        for i, b in enumerate(self.buildActions):\n",
    "            feature = b.feature\n",
    "            self.features[feature] = max(self.features[feature], observation[\"Build_progress\"][i])\n",
    "        \n",
    "        match self.transform:\n",
    "            case \"arctan\":\n",
    "                self.features = 2*np.arctan(self.features)/np.pi \n",
    "            case \"tanh\":\n",
    "                self.features = np.tanh(0.55 * self.features)   # Note: ln(3)/2 ~ 0.55\n",
    "            case _: \n",
    "                self.features = np.clip(self.features,1,0.5)\n",
    "        \n",
    "        for i, t in enumerate(self.testActions):\n",
    "            test = observation[\"Test_progress\"][i]\n",
    "            sen = t.attr2\n",
    "            spc = t.attr1\n",
    "            feature = t.feature\n",
    "            prior = self.features[feature]\n",
    "            if test == -1:\n",
    "                pass\n",
    "            elif test == 1:\n",
    "                self.features[feature] = (sen * prior) / (sen * prior + (1-spc) * (1 - prior))\n",
    "            else: # test == 0\n",
    "                self.features[feature] = ((1-sen) * prior) / ((1-sen) * prior + spc * (1-prior))\n",
    "\n",
    "        return {\n",
    "            \"Budget\": observation[\"Budget\"],\n",
    "            \"Features\": self.features,\n",
    "            \"Build_progress\": observation[\"Build_progress\"],\n",
    "            \"Test_progress\": observation[\"Test_progress\"]\n",
    "        }\n",
    "\n",
    "def evaluateDesign(design, driver):\n",
    "    env = edss.EDSSEnv(edss.design_from_yaml(design))\n",
    "    bayes1 = BayesEstimator(env)\n",
    "    bayes2 = BayesEstimator(env, transform=\"arctan\")\n",
    "    bayes3 = BayesEstimator(env, transform=\"tanh\")\n",
    "\n",
    "    exp_dir = design.removesuffix('.yaml')\n",
    "    mdp_agent = PPO.load(os.path.join(exp_dir, \"mdp_agent\"))\n",
    "    pomdp_agent = PPO.load(os.path.join(exp_dir, \"pomdp_agent\"))\n",
    "    #bayes1_agent = PPO.load(os.path.join(exp_dir, \"bayes_step_agent\"))\n",
    "    bayes2_agent = PPO.load(os.path.join(exp_dir, \"bayes_agent\"))\n",
    "    #bayes3_agent = PPO.load(os.path.join(exp_dir, \"bayes_tanh_agent\"))\n",
    "    recurrent_agent = RecurrentPPO.load(os.path.join(exp_dir, \"recurrent_agent\"))\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    episode_starts = torch.ones((0,), dtype=bool)\n",
    "    _, recurrent_states = recurrent_agent.predict({\"Budget\": obs[\"Budget\"], \"Build_progress\": obs[\"Build_progress\"], \"Test_progress\": obs[\"Test_progress\"]}, episode_start=np.ones((1,),dtype=bool))\n",
    "    recurrent_states_tensor = (torch.tensor(recurrent_states[0]), torch.tensor(recurrent_states[1]))\n",
    "    done = False\n",
    "    #idx = [\"MDP\", \"POMDP\", \"Bayes step\", \"Bayes arctan\", \"Bayes tahn\", \"Recurrent\"]\n",
    "    idx = [\"MDP\", \"POMDP\", \"Bayes arctan\", \"Recurrent\"]\n",
    "    columns = []\n",
    "    fea_cols = []\n",
    "    bel1_cols = []\n",
    "    bel2_cols = []\n",
    "    bel3_cols = []\n",
    "    bi_cols = []\n",
    "    ti_cols = []\n",
    "    for i in range(len(obs[\"Features\"])):\n",
    "        fea_cols.append(f'FS{i+1}')\n",
    "        bel1_cols.append(f'BsS{i+1}')\n",
    "        bel2_cols.append(f'BaS{i+1}')\n",
    "        bel3_cols.append(f'BtS{i+1}')\n",
    "    for i in range(len(obs[\"Build_progress\"])):\n",
    "        columns.append(f'BA{i+1}')\n",
    "        bi_cols.append(f'BP{i+1}')\n",
    "    for i in range(len(obs[\"Test_progress\"])):\n",
    "        columns.append(f'VA{i+1}')\n",
    "        ti_cols.append(f'VP{i+1}')\n",
    "    columns.append('Term')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while not done:\n",
    "            p_obs = {\"Budget\": obs[\"Budget\"], \"Build_progress\": obs[\"Build_progress\"], \"Test_progress\": obs[\"Test_progress\"]}\n",
    "\n",
    "            t_obs, _ = mdp_agent.policy.obs_to_tensor(obs)\n",
    "            mdp_dist = mdp_agent.policy.get_distribution(t_obs).distribution.probs\n",
    "        \n",
    "            t_obs, _ = pomdp_agent.policy.obs_to_tensor(p_obs)\n",
    "            pomdp_dist = pomdp_agent.policy.get_distribution(t_obs).distribution.probs\n",
    "\n",
    "            #t_obs, _ = bayes1_agent.policy.obs_to_tensor(bayes1.estimate(p_obs))\n",
    "            #bayes1_dist = bayes1_agent.policy.get_distribution(t_obs).distribution.probs\n",
    "\n",
    "            t_obs, _ = bayes2_agent.policy.obs_to_tensor(bayes2.estimate(p_obs))\n",
    "            bayes2_dist = bayes2_agent.policy.get_distribution(t_obs).distribution.probs\n",
    "\n",
    "            #t_obs, _ = bayes3_agent.policy.obs_to_tensor(bayes3.estimate(p_obs))\n",
    "            #bayes_dist = bayes3_agent.policy.get_distribution(t_obs).distribution.probs\n",
    "\n",
    "            t_obs, _ = recurrent_agent.policy.obs_to_tensor(p_obs)\n",
    "            recurrent_dist, recurrent_states_tensor = recurrent_agent.policy.get_distribution(t_obs, recurrent_states_tensor, episode_starts)\n",
    "            recurrent_dist = recurrent_dist.distribution.probs\n",
    "\n",
    "            budget_frame = pd.DataFrame([obs[\"Budget\"]], columns=[\"Budget\"])\n",
    "            feature_frame = pd.DataFrame([obs[\"Features\"]], columns=fea_cols)\n",
    "            #belief1_frame = pd.DataFrame([bayes1.estimate(p_obs)[\"Features\"]], columns=bel1_cols)\n",
    "            belief2_frame = pd.DataFrame([bayes2.estimate(p_obs)[\"Features\"]], columns=bel2_cols)\n",
    "            #belief3_frame = pd.DataFrame([bayes3.estimate(p_obs)[\"Features\"]], columns=bel3_cols)\n",
    "            build_frame = pd.DataFrame([obs[\"Build_progress\"]], columns=bi_cols)\n",
    "            test_frame = pd.DataFrame([obs[\"Test_progress\"]], columns=ti_cols)\n",
    "            df = budget_frame.join(feature_frame).join(belief2_frame).join(build_frame).join(test_frame)\n",
    "            display(df.style.hide(axis=\"index\"))\n",
    "            #df = pd.DataFrame([mdp_dist[0].numpy(), pomdp_dist[0].numpy(), bayes1_dist[0].numpy(), bayes2_dist[0].numpy(), bayes3_dist[0].numpy(), recurrent_dist[0].numpy()], index=idx, columns=columns)\n",
    "            df = pd.DataFrame([mdp_dist[0].numpy(), pomdp_dist[0].numpy(), bayes2_dist[0].numpy(), recurrent_dist[0].numpy()], index=idx, columns=columns)\n",
    "            s = df.style.highlight_max(axis=1, color=\"grey\")\n",
    "            display(s)\n",
    "            print()\n",
    "\n",
    "            match(driver):\n",
    "                case \"mdp\":\n",
    "                     action, _ = mdp_agent.predict(obs, deterministic=True)\n",
    "                case \"pomdp\":\n",
    "                     action, _ = pomdp_agent.predict(p_obs, deterministic=True)\n",
    "                #case \"bayes1\":\n",
    "                #    action, _ = bayes1_agent.predict(bayes1.estimate(p_obs), deterministic=True)\n",
    "                case \"bayes2\":\n",
    "                     action, _ = bayes2_agent.predict(bayes2.estimate(p_obs), deterministic=True)\n",
    "                #case \"bayes3\":\n",
    "                #     action, _ = bayes3_agent.predict(bayes3.estimate(p_obs), deterministic=True)\n",
    "                case \"recurrent\":\n",
    "                     action, recurrent_states = recurrent_agent.predict(p_obs, state=recurrent_states, deterministic=True)\n",
    "\n",
    "            obs, _, done, _, _ = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design = \"experiments/test1.yaml\"\n",
    "#design = \"experiments/build_pref.yaml\"\n",
    "#design = \"experiments/test_pref.yaml\"\n",
    "\n",
    "evaluateDesign(design=design, driver=\"mdp\")\n",
    "evaluateDesign(design=design, driver=\"pomdp\")\n",
    "evaluateDesign(design=design, driver=\"bayes2\")\n",
    "evaluateDesign(design=design, driver=\"recurrent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
